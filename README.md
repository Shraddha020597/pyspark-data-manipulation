Developed a comprehensive Google Colab notebook showcasing fundamental PySpark operations for data engineering. 

- Data Loading & Inspection: CSV, JSON, schema inference.
- Data Cleaning: Handling duplicates (latest record), filling nulls.
- DataFrame Transformations: Filtering, selecting, new columns, string manipulation, date functions.
- Data Aggregation: Grouping, summing, averaging, pivoting data.
- Advanced Analytics: Applying Window Functions for running totals.
- Spark SQL Integration: Querying DataFrames using SQL.
- User Defined Functions (UDFs): Custom data transformations.
- Data Security Concepts: Using MD5 and SHA2 hashing.
- Data Persistence: Writing partitioned Parquet files
