{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S8-DsDvFDfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa4e71d-2c51-48ad-8b24-c622311ff9bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm0D_w8wDcaH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NFrt0WCE9F2",
        "outputId": "d8418a5a-a897-48f9-918a-edc3c59be88d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 May 14 13:38 sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -ltr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEU6Eq5mGTzo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bd00b0e0-06c3-4a95-a2e1-61e7fdf5d52a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj9chZ0yPPgb",
        "outputId": "bb772d4a-ccd9-4980-da6a-92e973954f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIpxdX6jGaOA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e67374-c599-49ee-85cf-790ca7fb8786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-17 00:31:42--  https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobceles.github.io (jacobceles.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jacobceles.github.io (jacobceles.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv [following]\n",
            "--2025-05-17 00:31:42--  https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobcelestine.com (jacobcelestine.com)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jacobcelestine.com (jacobcelestine.com)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22608 (22K) [text/csv]\n",
            "Saving to: ‘cars.csv’\n",
            "\n",
            "cars.csv            100%[===================>]  22.08K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-05-17 00:31:43 (16.3 MB/s) - ‘cars.csv’ saved [22608/22608]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat cars.csv"
      ],
      "metadata": {
        "id": "GLaaVa9FV5mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"cars.csv\",sep=';',header=True)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us2CdQ09p6hf",
        "outputId": "13529c37-7087-4c47-cef8-fd411a7fb0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|Chevrolet Chevell...|18.0|        8|       307.0|     130.0| 3504.|        12.0|   70|    US|\n",
            "|   Buick Skylark 320|15.0|        8|       350.0|     165.0| 3693.|        11.5|   70|    US|\n",
            "|  Plymouth Satellite|18.0|        8|       318.0|     150.0| 3436.|        11.0|   70|    US|\n",
            "|       AMC Rebel SST|16.0|        8|       304.0|     150.0| 3433.|        12.0|   70|    US|\n",
            "|         Ford Torino|17.0|        8|       302.0|     140.0| 3449.|        10.5|   70|    US|\n",
            "|    Ford Galaxie 500|15.0|        8|       429.0|     198.0| 4341.|        10.0|   70|    US|\n",
            "|    Chevrolet Impala|14.0|        8|       454.0|     220.0| 4354.|         9.0|   70|    US|\n",
            "|   Plymouth Fury iii|14.0|        8|       440.0|     215.0| 4312.|         8.5|   70|    US|\n",
            "|    Pontiac Catalina|14.0|        8|       455.0|     225.0| 4425.|        10.0|   70|    US|\n",
            "|  AMC Ambassador DPL|15.0|        8|       390.0|     190.0| 3850.|         8.5|   70|    US|\n",
            "|Citroen DS-21 Pallas|   0|        4|       133.0|     115.0| 3090.|        17.5|   70|Europe|\n",
            "|Chevrolet Chevell...|   0|        8|       350.0|     165.0| 4142.|        11.5|   70|    US|\n",
            "|    Ford Torino (sw)|   0|        8|       351.0|     153.0| 4034.|        11.0|   70|    US|\n",
            "|Plymouth Satellit...|   0|        8|       383.0|     175.0| 4166.|        10.5|   70|    US|\n",
            "|  AMC Rebel SST (sw)|   0|        8|       360.0|     175.0| 3850.|        11.0|   70|    US|\n",
            "| Dodge Challenger SE|15.0|        8|       383.0|     170.0| 3563.|        10.0|   70|    US|\n",
            "|  Plymouth 'Cuda 340|14.0|        8|       340.0|     160.0| 3609.|         8.0|   70|    US|\n",
            "|Ford Mustang Boss...|   0|        8|       302.0|     140.0| 3353.|         8.0|   70|    US|\n",
            "|Chevrolet Monte C...|15.0|        8|       400.0|     150.0| 3761.|         9.5|   70|    US|\n",
            "|Buick Estate Wago...|14.0|        8|       455.0|     225.0| 3086.|        10.0|   70|    US|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgeKH4KPqJ8k",
        "outputId": "023f5c5b-2dfd-414d-ce8e-963c81caa6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Car: string (nullable = true)\n",
            " |-- MPG: double (nullable = true)\n",
            " |-- Cylinders: integer (nullable = true)\n",
            " |-- Displacement: double (nullable = true)\n",
            " |-- Horsepower: double (nullable = true)\n",
            " |-- Weight: decimal(4,0) (nullable = true)\n",
            " |-- Acceleration: double (nullable = true)\n",
            " |-- Model: integer (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ_c_XT0GkQC",
        "outputId": "f7943ce7-2828-40f1-dc85-2b19bcdeba99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------+-----+------+------+------+-----+------+------+-----+------+------+------+\n",
            "| indic_de,geo\\time| 2008 |2009 | 2010 | 2011 | 2012 |2013 | 2014 | 2015 |2016 | 2017 | 2018 | 2019 |\n",
            "+------------------+------+-----+------+------+------+-----+------+------+-----+------+------+------+\n",
            "|       PC_Y0_14,AD| 14.6 |14.5 | 14.5 | 15.5 | 15.5 |15.5 |    : |    : |   : |    : |    : | 13.9 |\n",
            "|       PC_Y0_14,AL| 24.1 |23.3 | 22.5 | 21.6 | 20.7 |20.1 | 19.6 | 19.0 |18.5 | 18.2 | 17.7 | 17.2 |\n",
            "|       PC_Y0_14,AM| 19.0 |18.6 | 18.3 |    : |    : |   : |    : | 19.4 |19.6 | 20.0 | 20.2 | 20.2 |\n",
            "|       PC_Y0_14,AT| 15.4 |15.1 | 14.9 | 14.7 | 14.6 |14.4 | 14.3 | 14.3 |14.3 | 14.4 | 14.4 | 14.4 |\n",
            "|       PC_Y0_14,AZ| 23.2 |22.6 | 22.6 | 22.3 | 22.2 |22.3 | 22.4 | 22.4 |22.5 | 22.6 | 22.6 | 22.4 |\n",
            "|       PC_Y0_14,BE| 16.9 |16.9 | 16.9 |17.0 b| 17.0 |17.0 | 17.0 | 17.0 |17.0 | 17.0 | 17.0 | 16.9 |\n",
            "|       PC_Y0_14,BG| 13.1 |13.1 | 13.2 | 13.2 | 13.4 |13.6 | 13.7 | 13.9 |14.0 | 14.1 | 14.2 | 14.4 |\n",
            "|       PC_Y0_14,BY| 14.7 |14.6 |    : | 14.9 | 15.1 |15.4 | 15.7 | 16.0 |16.3 | 16.6 | 16.8 | 16.9 |\n",
            "|       PC_Y0_14,CH| 15.5 |15.3 | 15.2 |15.1 b| 15.0 |14.9 | 14.9 | 14.9 |14.9 | 14.9 | 15.0 | 15.0 |\n",
            "|       PC_Y0_14,CY| 18.2 |17.7 | 17.2 | 16.8 | 16.5 |16.4 | 16.3 | 16.4 |16.4 | 16.3 | 16.2 | 16.1 |\n",
            "|       PC_Y0_14,CZ| 14.2 |14.2 | 14.3 | 14.5 | 14.7 |14.8 | 15.0 | 15.2 |15.4 | 15.6 | 15.7 | 15.9 |\n",
            "|       PC_Y0_14,DE| 13.7 |13.6 | 13.5 |13.6 b| 13.4 |13.3 |13.2 b| 13.2 |13.2 | 13.4 | 13.5 | 13.6 |\n",
            "|       PC_Y0_14,DK| 18.4 |18.3 | 18.1 | 17.9 | 17.7 |17.4 | 17.2 | 17.0 |16.8 | 16.7 | 16.6 | 16.5 |\n",
            "|     PC_Y0_14,EA18| 15.4 |15.4 | 15.4 |15.4 b|15.4 b|15.3 |15.3 b|15.2 b|15.1 |15.1 b|15.0 p|15.0 p|\n",
            "|     PC_Y0_14,EA19| 15.5 |15.5 | 15.4 |15.5 b|15.4 b|15.4 |15.3 b|15.3 b|15.2 |15.2 b|15.1 p|15.0 p|\n",
            "|       PC_Y0_14,EE| 14.8 |14.9 | 15.1 | 15.3 | 15.5 |15.7 | 15.8 |15.9 b|16.1 | 16.2 | 16.3 | 16.4 |\n",
            "|       PC_Y0_14,EL| 14.6 |14.6 | 14.6 | 14.6 | 14.7 |14.7 | 14.6 | 14.5 |14.4 | 14.4 | 14.4 | 14.3 |\n",
            "|       PC_Y0_14,ES| 14.6 |14.8 | 14.9 | 15.0 | 15.1 |15.2 | 15.2 | 15.2 |15.1 | 15.1 | 15.0 | 14.8 |\n",
            "|PC_Y0_14,EU27_2007|15.8 b|15.7 |15.7 b|15.7 b|15.7 b|15.7 |15.6 b|15.6 b|15.6 |15.6 b|15.6 p|15.5 p|\n",
            "|PC_Y0_14,EU27_2020|15.5 b|15.4 |15.4 b|15.4 b|15.4 b|15.4 |15.3 b|15.3 b|15.3 |15.2 b|15.2 p|15.2 p|\n",
            "+------------------+------+-----+------+------+------+-----+------+------+-----+------+------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(\"population_by_age.tsv\",sep=\"\\t\",header=True,inferSchema=True)\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrlyswsLHsIP",
        "outputId": "a879686a-57e0-4810-d5c6-cd7a6792fbc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Car: string (nullable = true)\n",
            " |-- MPG: double (nullable = true)\n",
            " |-- Cylinders: integer (nullable = true)\n",
            " |-- Displacement: double (nullable = true)\n",
            " |-- Horsepower: double (nullable = true)\n",
            " |-- Weight: decimal(4,0) (nullable = true)\n",
            " |-- Acceleration: double (nullable = true)\n",
            " |-- Model: integer (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xYsL8TiIfMX",
        "outputId": "dcdb31e2-4699-45a5-d51d-187a98141859"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "406"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzidAuNaIsQE",
        "outputId": "6dcf3116-2308-471b-c31d-74785bedc75a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Car: string, MPG: string, Cylinders: string, Displacement: string, Horsepower: string, Weight: string, Acceleration: string, Model: string, Origin: string]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk170MNzIvxb",
        "outputId": "eae11267-3c25-485a-ae9f-7dac26e4da0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Car', 'MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model', 'Origin']\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zbH7evfI32Z",
        "outputId": "35ef3dc9-40ee-4782-98db-c705de29707c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+------------+\n",
            "| id| name|   loc|undated_date|\n",
            "+---+-----+------+------------+\n",
            "|  1| John|   USA|  2022-01-25|\n",
            "|  1| John|Mexico|  2023-04-22|\n",
            "|  1| John|Canada|  2023-04-25|\n",
            "|  2|Alice|Canada|  2023-04-24|\n",
            "|  3|  Bob|    UK|  2023-04-23|\n",
            "+---+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_dup = spark.read.csv(\"my_duplicate_data.csv\",header=True)\n",
        "#df_dup = spark.read.format(\"csv\").load(\"my_duplicate_data.csv\")\n",
        "df_dup.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU22GmSXJ-nn",
        "outputId": "2789a773-c107-46a0-e1c9-8cda54d48af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+------------+\n",
            "| id| name|   loc|undated_date|\n",
            "+---+-----+------+------------+\n",
            "|  1| John|Canada|  2023-04-25|\n",
            "|  2|Alice|Canada|  2023-04-24|\n",
            "|  3|  Bob|    UK|  2023-04-23|\n",
            "+---+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "#df_dup.select(\"id\").show()\n",
        "#df_dup.select(col('id')).show()\n",
        "df_clean_data = df_dup.orderBy(col('undated_date').desc()).dropDuplicates(['id'])\n",
        "df_clean_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL4w4szv1XzA",
        "outputId": "19d215ec-3f34-416a-f0ee-7667e3e4ade5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+------------+------+\n",
            "| id|name|   loc|undated_date|row_id|\n",
            "+---+----+------+------------+------+\n",
            "|  1|John|Mexico|  2023-04-22|     2|\n",
            "|  1|John|   USA|  2022-01-25|     3|\n",
            "+---+----+------+------------+------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "df_dup.count()\n",
        "wSpec = Window.partitionBy('id').orderBy(col('undated_date').desc())\n",
        "\n",
        "df_temp = df_dup.withColumn('row_id',row_number().over(wSpec))#Window.partitionBy('id').orderBy(col('undated_date').desc())))\n",
        "\n",
        "df_temp = df_temp.filter(df_temp['row_id']!=1)\n",
        "\n",
        "df_temp.show()\n",
        "df_temp.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vp9B5EQKmmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23717861-672f-44cc-fb8e-6ec7da8ee744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------+----------+---+---+\n",
            "| _1|   _2|      _3|        _4| _5| _6|\n",
            "+---+-----+--------+----------+---+---+\n",
            "|  1| John|   Shoes|2022-01-01|  2|100|\n",
            "|  2|Peter|   Shirt|2022-01-02|  3| 50|\n",
            "|  3| Mary|   Shoes|2022-01-03|  1|150|\n",
            "|  4| John|   Shoes|2022-01-04|  2|100|\n",
            "|  5|Peter|Trousers|2022-01-05|  1| 75|\n",
            "|  6| Mary|   Shirt|2022-01-06|  2| 50|\n",
            "|  7| John|   Shoes|2022-01-07|  1|100|\n",
            "|  8|Peter|   Shirt|2022-01-08|  1| 50|\n",
            "|  9| Mary|Trousers|2022-01-09|  3| 75|\n",
            "| 10| John|Trousers|2022-01-10|  2| 75|\n",
            "+---+-----+--------+----------+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [\n",
        "    (1, \"John\", \"Shoes\", \"2022-01-01\", 2, 100),\n",
        "    (2, \"Peter\", \"Shirt\", \"2022-01-02\", 3, 50),\n",
        "    (3, \"Mary\", \"Shoes\", \"2022-01-03\", 1, 150),\n",
        "    (4, \"John\", \"Shoes\", \"2022-01-04\", 2, 100),\n",
        "    (5, \"Peter\", \"Trousers\", \"2022-01-05\", 1, 75),\n",
        "    (6, \"Mary\", \"Shirt\", \"2022-01-06\", 2, 50),\n",
        "    (7, \"John\", \"Shoes\", \"2022-01-07\", 1, 100),\n",
        "    (8, \"Peter\", \"Shirt\", \"2022-01-08\", 1, 50),\n",
        "    (9, \"Mary\", \"Trousers\", \"2022-01-09\", 3, 75),\n",
        "    (10, \"John\", \"Trousers\", \"2022-01-10\", 2, 75)\n",
        "]\n",
        "\n",
        "df_h = spark.createDataFrame(data)\n",
        "df_h.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJkgVD6eNVDE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", IntegerType(),True),\n",
        "    StructField(\"cust_name\", StringType(),True),\n",
        "    StructField(\"product_name\", StringType(),True),\n",
        "    StructField(\"date\", StringType(),True),\n",
        "    StructField(\"quantity\", IntegerType(),True),\n",
        "    StructField(\"price\", IntegerType(),True),\n",
        "]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBN5N9yHOel6",
        "outputId": "70528ebc-b232-4aa7-a492-9e1bd6782bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+------------+----------+--------+-----+\n",
            "|order_id|cust_name|product_name|      date|quantity|price|\n",
            "+--------+---------+------------+----------+--------+-----+\n",
            "|       1|     John|       Shoes|2022-01-01|       2|  100|\n",
            "|       2|    Peter|       Shirt|2022-01-02|       3|   50|\n",
            "|       3|     Mary|       Shoes|2022-01-03|       1|  150|\n",
            "|       4|     John|       Shoes|2022-01-04|       2|  100|\n",
            "|       5|    Peter|    Trousers|2022-01-05|       1|   75|\n",
            "|       6|     Mary|       Shirt|2022-01-06|       2|   50|\n",
            "|       7|     John|       Shoes|2022-01-07|       1|  100|\n",
            "|       8|    Peter|       Shirt|2022-01-08|       1|   50|\n",
            "|       9|     Mary|    Trousers|2022-01-09|       3|   75|\n",
            "|      10|     John|    Trousers|2022-01-10|       2|   75|\n",
            "+--------+---------+------------+----------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_h = spark.createDataFrame(data,schema)\n",
        "df_h.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oNxOFkFPQLi",
        "outputId": "999d34e1-c2e0-47cf-edcd-ebef825407ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|product_name|\n",
            "+------------+\n",
            "|    Trousers|\n",
            "|       Shirt|\n",
            "|       Shoes|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_h.select(\"product_name\").distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtwEyjYePXAn",
        "outputId": "7f5c1e68-4e5e-4188-db2e-6c8330984305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+------------+----------+--------+-----+\n",
            "|order_id|cust_name|product_name|      date|quantity|price|\n",
            "+--------+---------+------------+----------+--------+-----+\n",
            "|       1|     John|       Shoes|2022-01-01|       2|  100|\n",
            "|       3|     Mary|       Shoes|2022-01-03|       1|  150|\n",
            "|       4|     John|       Shoes|2022-01-04|       2|  100|\n",
            "|       7|     John|       Shoes|2022-01-07|       1|  100|\n",
            "+--------+---------+------------+----------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_h.filter(\"price > 90\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_E5CVtePzPG",
        "outputId": "0d9ea0bd-9f41-4a05-bc00-bd887a0fefda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-12 22:34:59--  https://raw.githubusercontent.com/pathwaytipsorg/gcp-data-engineering/04340347ed64fc60d35f9d746dcf3384f96b00d6/dproc/data/emp1.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 221 [text/plain]\n",
            "Saving to: ‘emp1.json’\n",
            "\n",
            "\remp1.json             0%[                    ]       0  --.-KB/s               \remp1.json           100%[===================>]     221  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-12 22:35:00 (13.7 MB/s) - ‘emp1.json’ saved [221/221]\n",
            "\n",
            "cars.csv  emp1.json  my_duplicate_data.csv  sample_data\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/pathwaytipsorg/gcp-data-engineering/04340347ed64fc60d35f9d746dcf3384f96b00d6/dproc/data/emp1.json\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O_Z0IzoRB6S",
        "outputId": "495a6a77-b239-4ec7-b54e-93d201476d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"name\":\"AliceSmith\",\"age\":25,\"address\":{\"city\":\"LosAngeles\",\"state\":\"CA\"}},\n",
            "{\"name\":\"BobJohnson\",\"age\":40,\"address\":{\"city\":\"Chicago\",\"state\":\"IL\"}},\n",
            "{\"name\":\"JohnDoe\",\"age\":30,\"address\":{\"city\":\"NewYork\",\"state\":\"NY\"}}\n"
          ]
        }
      ],
      "source": [
        "!cat emp1.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaTxgfMPO0SE",
        "outputId": "3662e58e-ac97-44cc-e469-63f5054d69df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-------------------+\n",
            "|   name|age|            address|\n",
            "+-------+---+-------------------+\n",
            "|   John| 30|     {New York, NY}|\n",
            "|   Jane| 25|{San Francisco, CA}|\n",
            "|Michael| 12|  {Los Angeles, CA}|\n",
            "+-------+---+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "schema1 = StructType([\n",
        "    StructField(\"name\",StringType(),True),\n",
        "    StructField(\"age\",IntegerType(),True),\n",
        "    StructField(\"address\",StructType([\n",
        "        StructField(\"city\",StringType(),True),\n",
        "        StructField(\"state\",StringType(),True)\n",
        "    ]),True)\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (\"John\", 30, (\"New York\", \"NY\")),\n",
        "    (\"Jane\", 25, (\"San Francisco\", \"CA\")),\n",
        "    (\"Michael\", 12, (\"Los Angeles\", \"CA\"))\n",
        "]\n",
        "\n",
        "js_df = spark.read.schema(schema1).json(\"emp1.json\")\n",
        "\n",
        "js_df = spark.createDataFrame(data, schema1)\n",
        "\"\"\"\n",
        "js_df = spark.read.json(\"emp1.json\")\n",
        "\"\"\"\n",
        "js_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxs6vEalO-mW",
        "outputId": "46717e70-f9c3-437f-f10d-af87e706cf25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-------------------+----------------+\n",
            "|   name|age|            address|     new_address|\n",
            "+-------+---+-------------------+----------------+\n",
            "|   John| 30|     {New York, NY}|     New York:NY|\n",
            "|   Jane| 25|{San Francisco, CA}|San Francisco:CA|\n",
            "|Michael| 12|  {Los Angeles, CA}|  Los Angeles:CA|\n",
            "+-------+---+-------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "js_df = js_df.withColumn(\"new_address\",concat_ws(\":\",js_df.address.city,js_df.address.state))\n",
        "js_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7wOO22rSqPb",
        "outputId": "1238a9d4-d4e3-4f93-c45b-005593b7612f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---+----------+-------------+--------+\n",
            "|         address|age|      name|  new_address|is_minor|\n",
            "+----------------+---+----------+-------------+--------+\n",
            "|{LosAngeles, CA}| 25|AliceSmith|LosAngeles:CA|      No|\n",
            "|   {Chicago, IL}| 40|BobJohnson|   Chicago:IL|      No|\n",
            "|   {NewYork, NY}| 30|   JohnDoe|   NewYork:NY|      No|\n",
            "+----------------+---+----------+-------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "js_df = js_df.withColumn(\"is_minor\",when(col(\"age\") < 18,\"Yes\").otherwise(\"No\"))\n",
        "js_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C81LiicBT3yi",
        "outputId": "0faef4aa-144d-431d-c4d9-7928252502df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31.666666666666668"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "#avg1 = js_df.select(avg(\"age\")).first()[0]\n",
        "js_df.select(avg(\"age\")).collect()[0][0]\n",
        "#js_df.select(avg(\"age\")).first()[0]\n",
        "#print(avg1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rbsbGrIXjAv",
        "outputId": "89e11c88-4b15-40e1-fa0c-d2bfbd295717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|state|people_count|\n",
            "+-----+------------+\n",
            "|   NY|           1|\n",
            "|   CA|           2|\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "country_by_state = js_df.groupBy(\"address.state\").agg(count(\"*\").alias(\"people_count\"))\n",
        "country_by_state.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64RnnVdiZG-y",
        "outputId": "64ec87d7-0fb7-4023-99c0-e3b81888051d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-------------------+----------------+\n",
            "|   name|age|            address|     new_address|\n",
            "+-------+---+-------------------+----------------+\n",
            "|   John| 30|     {New York, NY}|     New York:NY|\n",
            "|   Jane| 25|{San Francisco, CA}|San Francisco:CA|\n",
            "|Michael| 12|  {Los Angeles, CA}|  Los Angeles:CA|\n",
            "+-------+---+-------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "js_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg7vM-JvZ04z",
        "outputId": "3ca7057d-8c35-4a07-e11c-24f0ce13ddfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "|state|people|\n",
            "+-----+------+\n",
            "|   NY|     1|\n",
            "|   CA|     2|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "country_by_state.withColumnRenamed(\"people_count\",\"people\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVwAKKG2aBBD"
      },
      "outputs": [],
      "source": [
        "js_df.createOrReplaceTempView(\"employee\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZSHxLMuagPi",
        "outputId": "d37623de-47a2-4c0d-db1d-a31034704100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-------------------+----------------+\n",
            "|   name|age|            address|     new_address|\n",
            "+-------+---+-------------------+----------------+\n",
            "|   John| 30|     {New York, NY}|     New York:NY|\n",
            "|   Jane| 25|{San Francisco, CA}|San Francisco:CA|\n",
            "|Michael| 12|  {Los Angeles, CA}|  Los Angeles:CA|\n",
            "+-------+---+-------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from employee\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWy0RkB7alhS",
        "outputId": "846227c1-a4c3-49ed-ff7b-0bc07690a3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+--------------------+\n",
            "|      cc| name|             cc_hash|\n",
            "+--------+-----+--------------------+\n",
            "|11112222| John|821f3157e1a3456bf...|\n",
            "|22222333|Brand|139d85d84e8ec4608...|\n",
            "|33335436|Leela|f81126f8abb46878c...|\n",
            "+--------+-----+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# md5 (128)\n",
        "from pyspark.sql.functions import md5\n",
        "\n",
        "data = [\n",
        "    (11112222, \"John\"), (22222333, \"Brand\"), (33335436, \"Leela\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data,[\"cc\",\"name\"])\n",
        "\n",
        "df = df.withColumn(\"cc_hash\",md5(df[\"cc\"].cast(\"string\")))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRLP7dOXtj65",
        "outputId": "4f327fe0-8f34-436a-8dc9-52935ac16fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+--------------------+\n",
            "|      cc| name|             cc_hash|\n",
            "+--------+-----+--------------------+\n",
            "|11112222| John|0554a5df02ee12f1a...|\n",
            "|22222333|Brand|71daae1a4128dd237...|\n",
            "|33335436|Leela|30f56c80b200a5218...|\n",
            "+--------+-----+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# sha2\n",
        "from pyspark.sql.functions import sha2\n",
        "\n",
        "data = [\n",
        "    (11112222, \"John\"), (22222333, \"Brand\"), (33335436, \"Leela\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data,[\"cc\",\"name\"])\n",
        "\n",
        "df = df.withColumn(\"cc_hash\",sha2(df[\"cc\"].cast(\"string\"),256))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8PLuNiBv25Y",
        "outputId": "72b7eeb3-5338-4ee2-9e65-4b7dc40c460b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-12 23:06:18--  https://raw.githubusercontent.com/pathwaytipsorg/gcp-data-engineering/master/dproc/data/sales.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 225 [text/plain]\n",
            "Saving to: ‘sales.csv’\n",
            "\n",
            "\rsales.csv             0%[                    ]       0  --.-KB/s               \rsales.csv           100%[===================>]     225  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-12 23:06:18 (2.66 MB/s) - ‘sales.csv’ saved [225/225]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/pathwaytipsorg/gcp-data-engineering/master/dproc/data/sales.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo609RUUv42m",
        "outputId": "1d348779-2e95-48e2-92fe-8548929c596f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Company,Person,Sales\n",
            "ABC Inc.,John,500\n",
            "XYZ Ltd.,Jane,1000\n",
            "Acme Co.,Bob,750\n",
            "Acme Co.,Tina,\n",
            "Globex Corp.,Alice,1250\n",
            "GHI Co.,Peter,450\n",
            "GHI Co.,Ray,\n",
            "Acme Co.,Samantha,800\n",
            "PQR Corp.,David,1200\n",
            "PQR Corp.,,1211\n",
            "LMN Ltd.,Jessica,950\n"
          ]
        }
      ],
      "source": [
        "!cat sales.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUk53YVwv62w",
        "outputId": "1c73c46f-6dbb-4418-aa4f-6c202c5b8d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------+-----+\n",
            "|     Company|  Person|Sales|\n",
            "+------------+--------+-----+\n",
            "|    ABC Inc.|    John|  500|\n",
            "|    XYZ Ltd.|    Jane| 1000|\n",
            "|    Acme Co.|     Bob|  750|\n",
            "|    Acme Co.|    Tina| null|\n",
            "|Globex Corp.|   Alice| 1250|\n",
            "|     GHI Co.|   Peter|  450|\n",
            "|     GHI Co.|     Ray| null|\n",
            "|    Acme Co.|Samantha|  800|\n",
            "|   PQR Corp.|   David| 1200|\n",
            "|   PQR Corp.|    null| 1211|\n",
            "|    LMN Ltd.| Jessica|  950|\n",
            "+------------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_n = spark.read.csv(\"sales.csv\", header=True, inferSchema=True) #, inferSchema=True\n",
        "\n",
        "df_n.show()\n",
        "#df_n.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeU1rJznwiTY",
        "outputId": "880f9037-6fac-43b6-d1ad-caa1ad63f570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------+-----+\n",
            "|     Company|  Person|Sales|\n",
            "+------------+--------+-----+\n",
            "|    ABC Inc.|    John|  500|\n",
            "|    XYZ Ltd.|    Jane| 1000|\n",
            "|    Acme Co.|     Bob|  750|\n",
            "|    Acme Co.|    Tina|  901|\n",
            "|Globex Corp.|   Alice| 1250|\n",
            "|     GHI Co.|   Peter|  450|\n",
            "|     GHI Co.|     Ray|  901|\n",
            "|    Acme Co.|Samantha|  800|\n",
            "|   PQR Corp.|   David| 1200|\n",
            "|   PQR Corp.|    null| 1211|\n",
            "|    LMN Ltd.| Jessica|  950|\n",
            "+------------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "val_mean = df_n.select(mean(df_n[\"Sales\"])).collect()[0][0]\n",
        "\n",
        "val_mean\n",
        "\n",
        "df_n.na.fill(val_mean,[\"Sales\"]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LItfAYlazYG1",
        "outputId": "d06f18aa-7d7d-44b9-8294-a6bbb2d97bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+------------+-----+\n",
            "|customer_id|order_id|order_amount| name|\n",
            "+-----------+--------+------------+-----+\n",
            "|        101|       1|          50| John|\n",
            "|        101|       3|          75| John|\n",
            "|        102|       2|         100|Alice|\n",
            "|        102|       5|          60|Alice|\n",
            "|        103|       4|         120|  Bob|\n",
            "+-----------+--------+------------+-----+\n",
            "\n",
            "+-----------+-----+------------+\n",
            "|customer_id| name|Total_amount|\n",
            "+-----------+-----+------------+\n",
            "|        101| John|         125|\n",
            "|        102|Alice|         160|\n",
            "|        103|  Bob|         120|\n",
            "+-----------+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum\n",
        "orders_data = [(1, 101, 50),\n",
        "               (2, 102, 100),\n",
        "               (3, 101, 75),\n",
        "               (4, 103, 120),\n",
        "               (5, 102, 60)]\n",
        "order_columns = [\"order_id\", \"customer_id\", \"order_amount\"]\n",
        "orders = spark.createDataFrame(orders_data,order_columns)\n",
        "\n",
        "customers_data = [(101, \"John\"),\n",
        "                  (102, \"Alice\"),\n",
        "                  (103, \"Bob\")]\n",
        "customers_columns = [\"customer_id\",\"name\"]\n",
        "customers = spark.createDataFrame(customers_data,customers_columns)\n",
        "\n",
        "joned_df = orders.join(customers, on=\"customer_id\")\n",
        "joned_df.show()\n",
        "#joned_df.select(sum(\"order_amount\")).show()\n",
        "joned_df.groupBy(\"customer_id\",\"name\").agg(sum(\"order_amount\").alias(\"Total_amount\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxW8gb2oJdzq",
        "outputId": "3f784069-6d97-49d6-d184-e57517ec0c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+-----------------+\n",
            "|year|month|sum(order_amount)|\n",
            "+----+-----+-----------------+\n",
            "|2023|    3|               60|\n",
            "|2023|    2|              195|\n",
            "|2023|    1|              150|\n",
            "+----+-----+-----------------+\n",
            "\n",
            "+-----------+--------+------------+----------+-----+----+-----+\n",
            "|customer_id|order_id|order_amount|      date| name|year|month|\n",
            "+-----------+--------+------------+----------+-----+----+-----+\n",
            "|        101|       1|          50|2023-01-15| John|2023|    1|\n",
            "|        101|       3|          75|2023-02-05| John|2023|    2|\n",
            "|        102|       2|         100|2023-01-20|Alice|2023|    1|\n",
            "|        102|       5|          60|2023-03-15|Alice|2023|    3|\n",
            "|        103|       4|         120|2023-02-10|  Bob|2023|    2|\n",
            "+-----------+--------+------------+----------+-----+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum, year, month\n",
        "orders_data = [(1, 101, 50,\"2023-01-15\"),\n",
        "               (2, 102, 100,\"2023-01-20\"),\n",
        "               (3, 101, 75,\"2023-02-05\"),\n",
        "               (4, 103, 120,\"2023-02-10\"),\n",
        "               (5, 102, 60,\"2023-03-15\")]\n",
        "order_columns = [\"order_id\", \"customer_id\", \"order_amount\",\"date\"]\n",
        "orders = spark.createDataFrame(orders_data,order_columns)\n",
        "\n",
        "customers_data = [(101, \"John\"),\n",
        "                  (102, \"Alice\"),\n",
        "                  (103, \"Bob\")]\n",
        "customers_columns = [\"customer_id\",\"name\"]\n",
        "customers = spark.createDataFrame(customers_data,customers_columns)\n",
        "\n",
        "# view & sql : assignment\n",
        "joned_df = orders.join(customers, on=\"customer_id\")\n",
        "\n",
        "joned_df = joned_df.withColumn(\"year\",year(\"date\")).withColumn(\"month\",month(\"date\"))\n",
        "#joned_df.show()\n",
        "\n",
        "res = joned_df.groupBy(\"year\",\"month\").agg(sum(\"order_amount\"))\n",
        "res.show()\n",
        "\n",
        "# Assignment Step 1\n",
        "joned_df.createOrReplaceTempView(\"tbl_revenue\")\n",
        "#tmp_df = spark.sql(\"select year, month, sum(order_amount) from tbl_revenue group by year, month\")\n",
        "tmp_df = spark.sql(\"select * from tbl_revenue\")\n",
        "tmp_df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6TDkXh-ONEm",
        "outputId": "800b8c5e-29ef-4a05-eea9-dfc962ecf31e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-----+-------+\n",
            "|  product|quantity|price|revenue|\n",
            "+---------+--------+-----+-------+\n",
            "|Product A|      10|   50|    500|\n",
            "|Product B|      15|   75|   1125|\n",
            "|Product A|       5|   60|    300|\n",
            "|Product C|       8|   90|    720|\n",
            "|Product B|      20|  100|   2000|\n",
            "+---------+--------+-----+-------+\n",
            "\n",
            "+---------+-------------+\n",
            "|  product|total_revenue|\n",
            "+---------+-------------+\n",
            "|Product A|          800|\n",
            "|Product B|         3125|\n",
            "|Product C|          720|\n",
            "+---------+-------------+\n",
            "\n",
            "Top selling product:  Product B\n",
            "Total revenue:  3125\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "data = [(\"Product A\", 10, 50),\n",
        "        (\"Product B\", 15, 75),\n",
        "        (\"Product A\", 5, 60),\n",
        "        (\"Product C\", 8, 90),\n",
        "        (\"Product B\", 20, 100)]\n",
        "columns = [\"product\", \"quantity\", \"price\"]\n",
        "\n",
        "df = spark.createDataFrame(data,columns)\n",
        "# view & sql : assignment\n",
        "df = df.withColumn(\"revenue\", col(\"quantity\") * col(\"price\"))\n",
        "df.show()\n",
        "\n",
        "df = df.groupBy(\"product\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
        "df.show()\n",
        "\n",
        "df = df.orderBy(col(\"total_revenue\").desc()).first()\n",
        "\n",
        "print(\"Top selling product: \",df[\"product\"])\n",
        "print(\"Total revenue: \",df[\"total_revenue\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7wIQPaEQxvV",
        "outputId": "35ff01bc-dfe3-459e-e54f-cd30ddef86fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|   name|upcase_name|\n",
            "+-------+-----------+\n",
            "|  Mohan|      MOHAN|\n",
            "|  Sikhi|      SIKHI|\n",
            "|Anthony|    ANTHONY|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "#UDF\n",
        "data = [(\"Mohan\",),(\"Sikhi\",),(\"Anthony\",)]\n",
        "schema = [\"name\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "#Defining\n",
        "def conv2upper(na):\n",
        "  return na.upper()\n",
        "\n",
        "#Register\n",
        "up_case_udf = udf(conv2upper, StringType())\n",
        "\n",
        "df = df.withColumn(\"upcase_name\",up_case_udf(\"name\"))\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U78-lXbuScG6",
        "outputId": "fdcbf869-ccf5-4a5e-e734-c13cdc22cb15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+-----+\n",
            "|year|month|sales|\n",
            "+----+-----+-----+\n",
            "|2023|    1|  100|\n",
            "|2023|    1|  150|\n",
            "|2023|    2|   75|\n",
            "|2023|    2|  120|\n",
            "|2023|    3|   60|\n",
            "+----+-----+-----+\n",
            "\n",
            "+----+---+---+---+\n",
            "|year|  1|  2|  3|\n",
            "+----+---+---+---+\n",
            "|2023|250|195| 60|\n",
            "+----+---+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "#from pyspark.sql.functions import sum\n",
        "\n",
        "data = [(2023, 1, 100),\n",
        "        (2023, 1, 150),\n",
        "        (2023, 2, 75),\n",
        "        (2023, 2, 120),\n",
        "        (2023, 3, 60)]\n",
        "cols = [\"year\",\"month\",\"sales\"]\n",
        "\n",
        "df = spark.createDataFrame(data,cols)\n",
        "df.show()\n",
        "\n",
        "pvt_df = df.groupBy(\"year\").pivot(\"month\").agg(F.sum(\"sales\"))\n",
        "pvt_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIw90PaCWWJO",
        "outputId": "5c9ecf4e-b1f6-4094-feaa-b1f20e8c24ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-----+-----+\n",
            "|      date|revenue|month|total|\n",
            "+----------+-------+-----+-----+\n",
            "|2023-01-01|    100|   01|  100|\n",
            "|2023-01-10|    150|   01|  250|\n",
            "|2023-02-05|     75|   02|   75|\n",
            "|2023-02-15|    120|   02|  195|\n",
            "|2023-03-01|     60|   03|   60|\n",
            "+----------+-------+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data = [(\"2023-01-01\", 100),\n",
        "        (\"2023-01-10\", 150),\n",
        "        (\"2023-02-05\", 75),\n",
        "        (\"2023-02-15\", 120),\n",
        "        (\"2023-03-01\", 60)]\n",
        "\n",
        "cols = [\"date\",\"revenue\"]\n",
        "\n",
        "df = spark.createDataFrame(data, cols)\n",
        "# spark.sql assignment\n",
        "df_mnts = df.withColumn(\"month\",col(\"date\").substr(6,2))\n",
        "\n",
        "win_spec = Window.partitionBy(\"month\").orderBy(\"date\")\n",
        "\n",
        "df_final = df_mnts.withColumn(\"total\",sum(\"revenue\").over(win_spec))\n",
        "df_final.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4yCbm8PaS4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad498e00-f551-4f2e-e5e3-a1d807b9bc7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+-----------------+------+------+-----+---+----------+---------+-------+------+-------+\n",
            "| id|gender|age|       occupation|height|weight|waist| bp|       bmi|heartrate|glucose|income|smoking|\n",
            "+---+------+---+-----------------+------+------+-----+---+----------+---------+-------+------+-------+\n",
            "|  1|  Male| 27|Software Engineer|   6.1|     6|   42|  6|Overweight|   120/80|     77|  4200|   None|\n",
            "|  2|  Male| 28|           Doctor|   6.2|     6|   60|  8|    Normal|   130/85|     75| 10000|   None|\n",
            "|  3|Female| 32|          Teacher|   7.0|     7|   30|  5|    Normal|   110/70|     72|  5000|   None|\n",
            "+---+------+---+-----------------+------+------+-----+---+----------+---------+-------+------+-------+\n",
            "\n",
            "+---+------+---+-----------------+------+------+-----+---+----------+---------+-------+------+-------+----------+\n",
            "| id|gender|age|       occupation|height|weight|waist| bp|       bmi|heartrate|glucose|income|smoking|heart_rate|\n",
            "+---+------+---+-----------------+------+------+-----+---+----------+---------+-------+------+-------+----------+\n",
            "|  1|  Male| 27|Software Engineer|   6.1|     6|   42|  6|Overweight|   120/80|     77|  4200|   None|       120|\n",
            "|  2|  Male| 28|           Doctor|   6.2|     6|   60|  8|    Normal|   130/85|     75| 10000|   None|       130|\n",
            "|  3|Female| 32|          Teacher|   7.0|     7|   30|  5|    Normal|   110/70|     72|  5000|   None|       110|\n",
            "+---+------+---+-----------------+------+------+-----+---+----------+---------+-------+------+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"Data Analysis\").getOrCreate()\n",
        "\n",
        "# Create a list of tuples representing your data\n",
        "data = [\n",
        "    (1, \"Male\", 27, \"Software Engineer\", 6.1, 6, 42, 6, \"Overweight\", \"120/80\", 77, 4200, \"None\"),\n",
        "    (2, \"Male\", 28, \"Doctor\", 6.2, 6, 60, 8, \"Normal\", \"130/85\", 75, 10000, \"None\"),\n",
        "    (3, \"Female\", 32, \"Teacher\", 7.0, 7, 30, 5, \"Normal\", \"110/70\", 72, 5000, \"None\")\n",
        "]\n",
        "\n",
        "# Create a schema (column names and data types)\n",
        "schema = [\"id\", \"gender\", \"age\", \"occupation\", \"height\", \"weight\", \"waist\", \"bp\", \"bmi\", \"heartrate\", \"glucose\", \"income\", \"smoking\"]\n",
        "\n",
        "# Create a DataFrame from the list and schema\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Extract numerical value using regular expression\n",
        "df = df.withColumn(\"heart_rate\", regexp_extract(df.heartrate, r\"(\\d+)\", 1).cast(\"int\"))\n",
        "\n",
        "# Display the DataFrame with the new column\n",
        "df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "output_path = \"/path/to/output\"\n",
        "current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "date_partition_path = f\"{output_path}/date={current_date}\"\n",
        "\n",
        "if os.path.exists(date_partition_path):\n",
        "    # Delete the existing partition\n",
        "    shutil.rmtree(date_partition_path)\n",
        "\n",
        "# Write the updated data\n",
        "df.write.partitionBy(\"date\").parquet(output_path)\n"
      ],
      "metadata": {
        "id": "NnYTcgqFmEH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DailyDataWrite\").getOrCreate()\n",
        "\n",
        "output_path = \"/path/to/output\"\n",
        "current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "date_partition_path = f\"{output_path}/date={current_date}\"\n",
        "\n",
        "# Delete the existing partition if it exists\n",
        "if os.path.exists(date_partition_path):\n",
        "    # Use Spark to delete the partition\n",
        "    spark.sql(f\"ALTER TABLE your_table_name DROP IF EXISTS PARTITION (date='{current_date}')\")\n",
        "\n",
        "# Write the DataFrame to the desired location in append mode\n",
        "df.write.partitionBy(\"date\").mode(\"append\").parquet(output_path)\n"
      ],
      "metadata": {
        "id": "-vXvzixtmlSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Delta Lake:\n",
        "If you're using Databricks, consider using Delta Lake,\n",
        "which provides ACID transactions and additional features like merge, upsert, and time travel.\"\"\"\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "delta_table_path = \"/path/to/delta-table/\"\n",
        "\n",
        "# Assuming `df` is your DataFrame\n",
        "if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
        "    delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
        "    delta_table.alias(\"existing\").merge(\n",
        "        df.alias(\"new\"),\n",
        "        \"existing.id = new.id\"\n",
        "    ).whenNotMatchedInsertAll().execute()\n",
        "else:\n",
        "    df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qKfsfJ4Jk-jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Write Partitioned Parquet Data\").getOrCreate()\n",
        "\n",
        "# Sample Data (assuming you have a DataFrame with 'sales_date' and 'sales_amount' columns)\n",
        "data = [\n",
        "    (\"2023-01-01\", 100),\n",
        "    (\"2023-01-02\", 150),\n",
        "    (\"2023-02-01\", 200),\n",
        "    (\"2024-01-01\", 300),\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "columns = [\"sales_date\", \"sales_amount\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Convert 'sales_date' column to date type (if it's a string)\n",
        "df = df.withColumn(\"sales_date\", col(\"sales_date\").cast(\"date\"))\n",
        "\n",
        "# Add year, month, and day columns based on 'sales_date'\n",
        "df = df.withColumn(\"year\", year(col(\"sales_date\"))) \\\n",
        "       .withColumn(\"month\", month(col(\"sales_date\"))) \\\n",
        "       .withColumn(\"day\", dayofmonth(col(\"sales_date\")))\n",
        "df.show()\n",
        "# Define the output path where the partitioned data will be written\n",
        "output_path = \"abfss://<container_name>@<storage_account_name>.dfs.core.windows.net/sales/\"\n",
        "output_path = \"/content/sales/\"\n",
        "# Write the DataFrame as partitioned Parquet files\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
        "    .parquet(output_path)\n",
        "\n",
        "# Stop SparkSession when done\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddZ1UU5zXA9L",
        "outputId": "5c9104d9-e39a-4f32-8b4e-5c1481176811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----+-----+---+\n",
            "|sales_date|sales_amount|year|month|day|\n",
            "+----------+------------+----+-----+---+\n",
            "|2023-01-01|         100|2023|    1|  1|\n",
            "|2023-01-02|         150|2023|    1|  2|\n",
            "|2023-02-01|         200|2023|    2|  1|\n",
            "|2024-01-01|         300|2024|    1|  1|\n",
            "+----------+------------+----+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/sales/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO5uJyU4ZXkL",
        "outputId": "6f409dd1-1275-4118-f649-5d2d13c571f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " _SUCCESS  'year=2023'\t'year=2024'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CoalesceExample\").getOrCreate()\n",
        "\n",
        "# Simulate loading a large dataset\n",
        "initial_data = spark.range(0, 1000000).withColumnRenamed(\"id\", \"number\")\n",
        "\n",
        "# Simulate an initial large number of partitions\n",
        "initial_data = initial_data.repartition(100)\n",
        "\n",
        "# Filter down the dataset (Assume this is the requirement)\n",
        "filtered_data = initial_data.filter(\"number < 10001\")\n",
        "\n",
        "# Before coalesce: Check the number of partitions\n",
        "print(f\"Partitions before coalesce: {filtered_data.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Calculate the number of rows\n",
        "row_count = filtered_data.count()\n",
        "print(f\"Calculate the number of rows - {row_count}\")\n",
        "\n",
        "# Decide on number of partitions based on row count\n",
        "if row_count > 100000:\n",
        "    num_partitions = 20\n",
        "elif row_count > 10000:\n",
        "    num_partitions = 10\n",
        "else:\n",
        "    num_partitions = 5\n",
        "\n",
        "# Coalesce to reduce the number of partitions\n",
        "coalesced_data = filtered_data.coalesce(num_partitions)\n",
        "\n",
        "# After coalesce: Check the number of partitions\n",
        "print(f\"Partitions after coalesce: {coalesced_data.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Show the data\n",
        "coalesced_data.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DIPcsrSh1P1Y",
        "outputId": "d6ce5926-a0a2-4f9b-b200-4fb8888d5a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions before coalesce: 100\n",
            "Calculate the number of rows - 10001\n",
            "Partitions after coalesce: 10\n",
            "+------+\n",
            "|number|\n",
            "+------+\n",
            "|  1351|\n",
            "|  7957|\n",
            "|  8382|\n",
            "|  1131|\n",
            "|  1567|\n",
            "|  9924|\n",
            "|  4628|\n",
            "|  7325|\n",
            "|  5512|\n",
            "|  2890|\n",
            "|  9713|\n",
            "|  3073|\n",
            "|  9653|\n",
            "|  1635|\n",
            "|  5983|\n",
            "|  9086|\n",
            "|  9354|\n",
            "|  2688|\n",
            "|  7083|\n",
            "|  5249|\n",
            "+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RepartitionExample\").getOrCreate()\n",
        "\n",
        "# Read the initial large dataset\n",
        "large_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\",\";\").load(\"cars.csv\")\n",
        "large_data.show(3)\n",
        "\n",
        "initial_partitions = large_data.rdd.getNumPartitions()\n",
        "print(f\"Initial number of partitions: {initial_partitions}\")\n",
        "\n",
        "# Calculate the number of rows\n",
        "row_count = large_data.count()\n",
        "\n",
        "# Decide on number of partitions based on file count\n",
        "if row_count < 1000:\n",
        "    num_partitions = 5\n",
        "elif row_count < 5000:\n",
        "    num_partitions = 10\n",
        "else:\n",
        "    num_partitions = 20\n",
        "\n",
        "# Repartition the dataframe to (num_partitions) partitions\n",
        "df_repartitioned = large_data.repartition(num_partitions)\n",
        "\n",
        "new_partitions = df_repartitioned.rdd.getNumPartitions()\n",
        "print(f\"Number of partitions after repartition: {new_partitions}\")\n",
        "\n",
        "# Example transformation\n",
        "transformed_data = df_repartitioned.filter(\"Car IS NOT NULL\")\n",
        "\n",
        "# Write the transformed data\n",
        "# transformed_data.write.format(\"parquet\").save(\"car_output\")\n",
        "\n",
        "# To see the data csv write\n",
        "transformed_data.write.mode(\"overwrite\").format(\"csv\").save(\"csv_output\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDsGIueJ4v8P",
        "outputId": "1b7f1ee0-fe81-444e-99ef-5cfe677d879f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|Chevrolet Chevell...|18.0|        8|       307.0|     130.0| 3504.|        12.0|   70|    US|\n",
            "|   Buick Skylark 320|15.0|        8|       350.0|     165.0| 3693.|        11.5|   70|    US|\n",
            "|  Plymouth Satellite|18.0|        8|       318.0|     150.0| 3436.|        11.0|   70|    US|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "only showing top 3 rows\n",
            "\n",
            "Initial number of partitions: 1\n",
            "Number of partitions after repartition: 5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}